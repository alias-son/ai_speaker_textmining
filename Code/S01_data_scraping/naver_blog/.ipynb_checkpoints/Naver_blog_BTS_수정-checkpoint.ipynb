{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 블로그 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blog URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_blogs(url):\n",
    "    results = []\n",
    "    urls = []\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    lis = soup.find_all('li', attrs={'class':'sh_blog_top', 'id':re.compile('sp_blog')})\n",
    "    \n",
    "    for li in lis:\n",
    "\n",
    "        title = li.find('a', attrs={'class':re.compile(r'_sp_each_title')}).get('title').strip()\n",
    "        if li.find('a', attrs={'class':re.compile(r'_sp_each_url')}).text:\n",
    "            naver_url = li.find('a', attrs={'class':re.compile(r'_sp_each_url')}).get('href').strip()\n",
    "#        pub = li.find('span', attrs={'class':'_sp_each_source'}).text.strip()\n",
    "#        date = li.find('span', attrs={'class':'bar'}).next_sibling.strip()\n",
    "        results.append([title, naver_url])\n",
    "        urls.append(naver_url)\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real data URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_realdata_url_naverblog(url):\n",
    "    \"\"\"\n",
    "    네이버 블로그 포스트의 실제 데이터가 들어있는 URL을 반환해줌.\n",
    "    보통 3가지 경우가 있는 것 같음.\n",
    "    <지금까지 찾은 경우의 수>\n",
    "    1. id='screenFrame'이 담긴 페이지에서 얻은 URL -> (base_url) + (id='mainFrame'이 담긴 페이지에서 얻은 URL) -> 실제 데이터가 담긴 URL\n",
    "    2. (base_url) + (id='mainFrame'이 담긴 페이지에서 얻은 URL) -> 실제 데이터가 담긴 URL\n",
    "    3. 검색해서 처음 얻은 URL이 실제 데이터가 들어가 있는 URL임\n",
    "    \"\"\"\n",
    "    base_url = \"http://blog.naver.com\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text.encode('utf-8'), 'html5lib')\n",
    "    \n",
    "    if soup.find(id='screenFrame'):\n",
    "        # id = 'screenFrame'이 존재하는 경우 (id: screenFrame URL에서 -> id: mainFrame URL로 넘김)\n",
    "        url = soup.find(id='screenFrame').get('src')\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text.encode('utf-8'), 'html5lib')\n",
    "    if soup.find(id='mainFrame'):\n",
    "        # id = 'mainFrame'이 존재하는 경우 (id: mainFrame URL에서 -> id: real data URL로 넘김)\n",
    "        url = base_url + soup.find(id='mainFrame').get('src')\n",
    "    return url   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title_naverblog(soup):\n",
    "    \"\"\"\n",
    "    BeautifulSoup 객체를 받아서 블로그 포스트의 제목을 반환해줌.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #블로그 포스트 제목 추출\n",
    "        title = soup.find('meta', attrs={'property':'og:title'}).get('content')\n",
    "    except:\n",
    "        title = 'NA'\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def det_publish_datetime_naverblog(soup):\n",
    "    \"\"\"\n",
    "    BeautifulSoup 객체를 받아서 블로그 포스트의 작성일을 반환해줌.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #블로그 작성일 추출\n",
    "        date_tag = soup.find('span', attrs={'class':'se_publishDate pcol2'})\n",
    "        if date_tag:\n",
    "            # <span> Tag의 se_publishDate pcol2 클래스 이름으로 작성일 추출이 가능한 경우\n",
    "            date_str = date_tag.get_text()\n",
    "        \n",
    "        else:\n",
    "            # <p> Tag의 date fil5 pcol2 _postAddDate 클래스 이름으로 작성일 추출이 가능한 경우\n",
    "            date_tag = soup.find('p', attrs={'class' : 'date fil5 pcol2 _postAddDate'})\n",
    "            if date_tag:\n",
    "                date_str = date_tag.get_text()  \n",
    "        try:\n",
    "            #블로그 작성시간이 년, 월, 일로 되어있을 경우\n",
    "            publish_datetime = datetime.datetime.strptime(date_str, '%Y. %m. %d. %H:%M')\n",
    "        except:\n",
    "            #블로그 작성시간이 N시간 전, 방금전 이렇게 표시될 경우\n",
    "            publish_datetime = str(datetime.datetime.now()).split('.')[0] + ' ' + date_str\n",
    "    except:\n",
    "        publish_datetime = 'NA'\n",
    "    return publish_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_naverblog(soup):\n",
    "    \"\"\"\n",
    "    BeautifulSoup 객체를 받아서 블로그 포스트의 본문 내용을 반환해줌.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #블로그 본문 추출\n",
    "        #보통은 <span> Tag에, id: 'SE'로 시작하는 곳에 본문이 담겨있음\n",
    "        temp = soup.findAll(\"span\", attrs= {'id': re.compile('^SE')})\n",
    "        \n",
    "        texts = []\n",
    "        for t in temp:\n",
    "            temp_text = t.get_text()\n",
    "            if temp_text != '\\u200b':\n",
    "                texts.append(temp_text)\n",
    "                    \n",
    "        if not texts:\n",
    "            # 블로그의 본문 텍스트가 전혀 추출되지 않았을 경우에 한번 더 시도해본다.\n",
    "            # <p class=\"se_textarea\"> 내에 존재하는 모든 본문 텍스트를 가져옴\n",
    "            \"\"\"\n",
    "            # <p class=\"se_textarea\"> 내의 <span> Tag에 본문이 들어가 있을 경우 \n",
    "            for p in soup.findAll(\"p\", attrs={'class': \"se_textarea\"}):\n",
    "                for span in p.findAll(\"span\"):\n",
    "                    temp_text = span.get_text()\n",
    "                    if temp_text:\n",
    "                        # temp_text에 '' 이렇게 빈 문자가 들어가있는 경우를 제외해주기 위함\n",
    "                        texts.append(temp_text)\n",
    "            \"\"\"\n",
    "            for p in soup.findAll(\"p\", attrs={'class': \"se_textarea\"}):\n",
    "                temp_text = p.get_text()\n",
    "                if temp_text:\n",
    "                        # temp_text에 '' 이렇게 빈 문자가 들어가있는 경우를 제외해주기 위함\n",
    "                        texts.append(temp_text)\n",
    "                        \n",
    "        if not texts:\n",
    "            # 위의 두 가지 방법으로 본문 텍스트 추출을 시도했음에도 텍스트가 전혀 추출되지 않을 경우\n",
    "            # <div> Tag 내의 id 값이 'postViewArea'이고,\n",
    "            # 다시 그 태그의 내부에 존재하는 <div> Tag들 속에 존재하는 모든 본문 텍스트를 가져옴\n",
    "            for div in soup.find('div', attrs={'id': 'postViewArea'}).find('div'):\n",
    "                temp_text = div.get_text()\n",
    "                if temp_text:\n",
    "                        # temp_text에 '' 이렇게 빈 문자가 들어가있는 경우를 제외해주기 위함\n",
    "                        texts.append(temp_text)\n",
    "        \n",
    "        texts = \"\\n\".join(texts)\n",
    "    except:\n",
    "        texts = 'NA'\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  naver URL 구성\n",
    "* where = post(블로그)<br/>\n",
    "* query = <br/>\n",
    "* st = sim(관련도순), date(최신순) <br/>\n",
    "* date_from = <br/>\n",
    "* date_to = <br/>\n",
    "* date_option = 0(전체), 2,3,4,5,6,7(1일,1주,1개월,6개월,1년), 8(직접입력) <br/>\n",
    "* srchby = all(전체), title(제목)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selenium으로 blog url 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['20171001', '20171002', '20171003', '20171004', '20171005', '20171006',\n",
       "       '20171007', '20171008', '20171009', '20171010',\n",
       "       ...\n",
       "       '20190421', '20190422', '20190423', '20190424', '20190425', '20190426',\n",
       "       '20190427', '20190428', '20190429', '20190430'],\n",
       "      dtype='object', length=577)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_pd = pd.date_range('2017-09-01', '2019-04-30').strftime(\"%Y%m%d\") # 네이버 클로바 출시일- 2017.10\n",
    "date_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5dc9faadcf43f1b7aec0364bc21312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=577), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = '\"카카오%20미니\"%20스피커'\n",
    "\n",
    "#pickle_name = q+'_'+startDate+'_'+endDate+'.p'\n",
    "total_url = []\n",
    "\n",
    "for date in tqdm_notebook(date_pd):\n",
    "    for page in range(1,100):\n",
    "        try:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "            url = 'https://search.naver.com/search.naver?date_from='+str(date)+'&date_option=8&date_to='+str(date)+'&dup_remove=1&nso=p%3Afrom'+str(date)+'to'+str(date)+'&query='+query+'&where=post&start='+str(page)\n",
    "\n",
    "            results = get_blogs(url)\n",
    "\n",
    "            if results[0] in total_url:\n",
    "                break\n",
    "\n",
    "            total_url = total_url + results\n",
    "        except:\n",
    "            break     \n",
    "\n",
    "#pickle.dump(total_results.open(pickle_name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2983"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(total_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66084a94b61f4f23b97428a7f69ede22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3005), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('kakaomini_speaker_url.txt', 'w', encoding = 'utf8') as s:\n",
    "    for url in tqdm_notebook(total_url):\n",
    "        s.write(url+'\\n')\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://blog.mzzang.net/blog/51895'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_url[257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_url.remove(total_url[257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://blog.dreamyoungs.com/2018/03/09/%EA%BF%88%EB%A7%8E%EC%9D%80%EC%B2%AD%EB%85%84%EB%93%A4-x-%EC%B9%B4%EC%B9%B4%EC%98%A4-%ED%8C%8C%ED%8A%B8%EB%84%88%EC%8A%A4%EA%B3%B5%EC%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%EC%8B%9C%EC%82%AC-%EC%84%A0/'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_url[1156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA\n",
      "NA\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "real_url = get_realdata_url_naverblog(total_url[257])\n",
    "r = requests.get(real_url)\n",
    "soup = BeautifulSoup(r.text.encode(\"utf-8\"), \"html.parser\")\n",
    "print(get_texts_naverblog(soup))\n",
    "print(get_title_naverblog(soup))\n",
    "print(det_publish_datetime_naverblog(soup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get real_url, title, datetime, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4,5]:\n",
    "    if i == 4:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(스크랩)로엔엔터테인먼트의 멜론, 인공지능 스피커 덕에 급성장 확실',\n",
       " '대세는 인공지능 스피커! 네이버 웨이브 VS 카카오미니',\n",
       " \"[2017' 칸광고제] 소셜 플랫폼의 새로운 진화. 페이스북 챗봇(Facebook Chatbot)을 활용한 브라질 10대 청소년들의 음주 예방/치료 캠페인 - Alcoholics Anonymous (AA) 'Anonymous Friend' (2017' 칸광고제/페이스북 어워즈 수상작)\",\n",
       " '공감수 순위 목록 2017년 10월 02일 기준 ',\n",
       " '조회수 순위 목록 2017년 10월 02일 기준  ',\n",
       " '카카오미니 완판!! 얼마나 좋길래??',\n",
       " '조회수 순위 목록 2017년 10월 03일 기준 ',\n",
       " '공감수 순위 목록 2017년 10월 03일 기준 ',\n",
       " \"누구? 아리아? 모두가 헷갈리는 AI 스피커의 '이름들' 한방에 정리해 줄게\",\n",
       " '[지나툰] 2017.09.18 :: 카카오미니',\n",
       " '조회수 순위 목록 2017년 10월 05일 기준 ',\n",
       " '카카오미니 본체 공개..미니멀 및 친숙함 지녀',\n",
       " '네이버 인공지능 스피커, 웨이브 사용기 2-활용성',\n",
       " '인공지능 AI 스피커가 탑재된 자동차, 네이버 웨이브 vs 카카오 미니',\n",
       " '조회수 순위 목록 2017년 10월 02일 주간 ',\n",
       " 'NA',\n",
       " '공감수 순위 목록 2017년 10월 02일 주간 ',\n",
       " '업종대표주 카카오 주목…대형 배당주도 막바지 랠리 가능성',\n",
       " '카카오, 국내 건설사들과 AI 기반 스마트홈 구축 ‘맞손’',\n",
       " 'KB증권: 삼성전기·삼성바이오로직스·유니퀘스트 외',\n",
       " '인터넷/게임-3Q17 Preview: 패러다임의 변화',\n",
       " '카카오, 삼성전자와 손잡고 스마트 가전 시장 진출',\n",
       " 'CE : 카톡으로 삼성 냉장고·세탁기·에어컨 제어★',\n",
       " '카톡, 에어컨 켜\"…카카오 AI, 삼성 생활가전에도 탑재',\n",
       " '카톡 메세지·카카오 음성인식 스피커로 삼성전자 가전제품 제어한다...카카오·삼성 MOU',\n",
       " '카카오 미니 스피커 정보 및 정식 판매일',\n",
       " '카카오...환골탈퇴를 시작하다',\n",
       " \"신규 개설된 '인공지능 스피커 소통방' 카톡 오픈채팅방 소개합니다!\",\n",
       " '카카오 미니 vs 네이버 웨이브가 고민인 당신에게',\n",
       " '규제·성장성 논란에 죽 쑤던 네이버, 주가 방향 트나',\n",
       " '인공지능 스피커 _ 카카오의 첫번째 스마트 스피커, 카카오미니 (KaKao MiNi)',\n",
       " '멜론, ‘에비츄’ 신상 소리 나는 이모티콘 제공',\n",
       " '인공지능 스피커 – 네이버 웨이브 (Naver WAVE)',\n",
       " '네이버, AI스피커 신제품 할인판매[파이낸셜신문]',\n",
       " '강남, 카카오프렌즈샵에서, 구경 한 날.',\n",
       " '“집 좀 시원하게 해줘” 카톡 문자 하나면 삼성전자 에어컨 가동',\n",
       " '[인공지능 AI 스마트 스피커] 네이버 WAVE (혹은 프렌즈)와 카카오미니',\n",
       " '네이버 인공지능 비서 스피커 프렌즈 가격 공개',\n",
       " '[Keyword & Issue] 추석 연휴 기간, 랜섬웨어에 보안 비상등',\n",
       " '카카오 미니 기능 알아봅시다.',\n",
       " '[Hivelab Study]  사물인터넷 국제전시회 2017 참관기',\n",
       " '미래성장산업과 4차산업',\n",
       " '말로 다 되는 ‘보이스 퍼스트’ 세상 열릴까 ',\n",
       " \"☞2017.10.19(목) 네이버 스마트스피커 '프렌즈' 공개 / 인포마크 12.53% 수익실현!\",\n",
       " '2017년 10월 19일 기업실적자료 정리',\n",
       " '별걸 다 만드는 카카오톡, 스마트 스피커 출시!',\n",
       " '[클릭 e종목]\"로엔, 유료가입자 증가에 AI스피커로 매출 확대 가능성…목표가↑\"',\n",
       " '인공지능 스피커 NUGU/로봇청소기',\n",
       " '그냥 잠이 하나도 안오는 밤 끄적끄적',\n",
       " \"'카카오미니'를 설정/제어하기 위한 앱, '헤이카카오' 출시!\",\n",
       " '마켓인사이드 - 2017년 10월 20일(금)  13:25기준',\n",
       " '[4차 산업 혁명 관련 유망 주식 분석 보고서]',\n",
       " \"실시간으로 언어를 번역해주는 이어폰 - 구글의 '픽셀 버드'\",\n",
       " '(굿디자인) 인공지능 스피커 대전 카카오 ‘카카오 미니’ 대 네이버 ‘웨이브’ [인포그래픽]',\n",
       " '미래를 보는 남자-로엔, 유료가입자 AI스피커 판매 증가 등에 매출 확대',\n",
       " '[E Daily]  AI 스피커 살까 고민 중이신가요?',\n",
       " \"'4차 산업혁명'과 의료산업에서 '인공지능(AI) 활용' 집중논의\",\n",
       " '포털, AI 스피커 이달 말 판매 대결',\n",
       " \"'AI 주도권 잡아라' 네이버·카카오·구글 진검승부\",\n",
       " '로엔',\n",
       " '2017년 10월 23일 뉴지스탁 유니버스 1등 종목 (주도주)',\n",
       " '20171023 이슈',\n",
       " '인공지능 스피커 누구(NUGU), 단점 3가지',\n",
       " '네이버 프렌즈 AI 스피커!  음성비스라 부른다!',\n",
       " '네이버 프렌즈 AI 스피커 10월 26일 출시 기다려볼까요~',\n",
       " '네이버, 카카오 AI 스피커 이달 말 판매 대결',\n",
       " '인문학특강 - 4차산업시대의 환경변화와 IT 트렌드 속 기회',\n",
       " '네이버 웨이브 기능을 알아볼까요.',\n",
       " 'SW기업들 HW 영토로…“인공지능 시대, 문제는 데이터야”',\n",
       " '우리나라 유망산업군은 어떤 것이 있을까?',\n",
       " '카카오 인공지능 스피커 카카오 미니 꿀잼 일상 활용',\n",
       " '스마트스피커 카카오미니 생활에 활력을주는 프렌즈스피커',\n",
       " '카카오미니 스마트 스피커 매력 뿜뿜',\n",
       " '카카오의 첫 번째 인공지능(AI) 스마트 스피커 카카오미니 후기~',\n",
       " '카카오미니 앙증맞은 라이언 스마트스피커 활용법~',\n",
       " '스마트한 카카오미니 프렌즈스피커 슈퍼 그뤠잇!',\n",
       " '카카오미니 스마트스피커 활용하기 굿!(프렌즈스피커)',\n",
       " '카카오미니 프렌즈 스마트 스피커 사용기~ 생활이 더 편해졌어요! ',\n",
       " '늘 가까이! 동반자 같은 카카오미니! 인공지능 스피커!',\n",
       " '카카오미니 스마트스피커 AI hey Kakao ! 활용 후기 ♩',\n",
       " '멜론 전용 스피커 카카오 미니, 3가지 장점은?',\n",
       " '스마트 스피커 카카오미니 후기. 기다렸어. Hey, Kakao!',\n",
       " '카카오미니 스피커 정말 똑똑해요. ',\n",
       " '너 누구니? 인공지능 스마트스피커, 카카오미니 등장!',\n",
       " '카카오프렌즈 스마트스피커 :: 카카오미니 라이언 사용 후기는? 활용도 굿 :)',\n",
       " '블루투스 스피커#카카오미니 개봉기!!',\n",
       " '카카오 미니, 스마트 인공지능 스피커 솔직 후기',\n",
       " '헤이카카오!-AI스피커 카카오미니 괜찮네요',\n",
       " '인공지능 스마트스피커 카카오미니 탁월한 성능',\n",
       " '인공지능 스피커 카카오미니 개봉기',\n",
       " '카카오미니 스피커 어피치 후기, 멜론 활용 200% 아이템!',\n",
       " '카카오미니 스피커 후기!!진짜 넘나 좋아용:)',\n",
       " '\"카카오미니가 도착했습니다_음질은 귀여움으로 돌파한다!\"',\n",
       " '캐주얼 모바일게임 프렌즈팝콘 1주년 생일상 어워즈 그레잇!',\n",
       " '카카오미니 개봉기, 후기',\n",
       " '[블로그 기자단] 귀여운 캐릭터의 무서운 인기, 캐릭터 비즈니스',\n",
       " '카카오 인공지능 스피커, 카카오미니 후기 들구 왔찌영 !!!!!!',\n",
       " '네이버 프렌즈 구매후기! 인공지능 AI 스피커 조으당',\n",
       " '프렌즈 스피커 브라운vs샐리',\n",
       " '네이버 인공지능 스피커를 만나보세요.',\n",
       " '스마트스피커 카카오미니 너무나 똑똑해!',\n",
       " '로봇^인공지능 관련기사 – 네이버 라인 스피커 ‘프렌즈’, 벌써 1만대 팔려',\n",
       " '이별 노래 틀어줘 ‘안녕 나의 사랑’ … 외계인 있니? 우주는 넓어요',\n",
       " \"이별 노래 틀어줘 '안녕 나의 사랑' … 외계인 있니? 우주는 넓어요\",\n",
       " '[동아인 꿀팁] 인공지능 스피커로 무엇을 할 수 있을까?',\n",
       " \"귀여운 인공지능 스피커! '카카오미니'가 왔습니다!\",\n",
       " '카카오의 첫 번째 AI 스피커 \"카카오 미니\" 개봉기 / 후기',\n",
       " \"[영진전문대 컴퓨터정보계열]4차산업혁명 '육아' 솔루션...AI 스피커, 스마트폰 중독 막는다\",\n",
       " '[구입기] Clova AI Speaker - Friends',\n",
       " '네이버 프렌즈 스피커(샐리) 구입했어요!',\n",
       " '카카오미니 도착',\n",
       " '행복한 일상들:-)',\n",
       " '10월 30일의 거시경제 Letter',\n",
       " '카카오미니 인공지능 스피커 리얼후기, hey kakao 반가워',\n",
       " '카카오미니 스마트스피커 카카오프렌즈 똑소리나는걸',\n",
       " '카카오미니 스마트스피커 너를 내 비서로 임명한다!! ㅎㅎㅎ',\n",
       " '[네이버 인공지능 스피커 프렌즈] AI 스피커 브라운이 드디어 내품안으로~',\n",
       " '[첫번째]네이버 클로바 인공지능 스피커 프렌즈 브라운',\n",
       " '네이버 프렌즈 스피커 수령했다. hi 셀리~',\n",
       " '오호 이거 물건일세... 네이버 클로바 프렌즈 스피커',\n",
       " '카카오미니 스피커 개봉 솔직 후기',\n",
       " 'AI스피커 대전 ㅡ출시예정 소식듣고 설렘',\n",
       " '인공지능스피커 카카오미니 11월7일 정식 발매',\n",
       " \"카카오, AI 스피커 '카카오미니' 11월7일 출시\",\n",
       " \"[다나와]카카오, AI 스피커 '카카오미니'가 11월 7일 출시\",\n",
       " \"카카오의 인공지능 스피커 '카카오미니', 11월7일 정식 출시 \",\n",
       " '카카오 미니(Kakao Mini)를 만나다.',\n",
       " 'AI 스피커의 진화, 그 끝은 어디인가?',\n",
       " '네이버 프렌즈 출시! 네이버의 두 번째 인공지능 스피커',\n",
       " '인공지능스피커프렌즈 구매기',\n",
       " '스마트 스피커 카카오미니(Kakao Mini) 11월 7일 정식 발매',\n",
       " '새친구 카카오미니를 만나다[개봉기]',\n",
       " '카카오 미니 11월 7일 정식 발매, 스펙과 가격 정보',\n",
       " '소셜미디어,사용자의 일상에 파고들다 / 디지털,웹 전문 매거진 월간DI(디지털 인사이트) 2017.11월호 매거진 기고',\n",
       " '카카오미니, 11월 7일(화)에 구매 가능!',\n",
       " '카카오미니 개봉기',\n",
       " '멜론 x 카카오 미니 인공지능 스피커 이벤트!',\n",
       " '인공지능 스피커 프렌즈',\n",
       " '스피커로 시작된 AI, 암 진단하고 자율주행차 굴린다',\n",
       " 'IT 블로거의 맥북프로 터치바 15인치 외장모니터 트리플 구성 (+작업실 배치도)',\n",
       " '[멜론-카카오] 카카오미니 2차 파격 행사, 정식발매',\n",
       " \"'사물인터넷 기기, 100% 뚫린다' - Byline Network\",\n",
       " \"LGU+-네이버, 늦어도 12.12까지 AI '스마트 스피커' 출시...'더 늦출 수 없다'\",\n",
       " '인공지능 스피커 클로바를 구입하다! 클로바 개봉&사용법',\n",
       " '샐리 스피커 / 네이버 프렌즈 / 인공지능 스피커 / 네이버 뮤직',\n",
       " '멜론 이용자라면 <카카오미니>특별가로 득템하세요~',\n",
       " '카카오미니 라이언캐릭터 구입',\n",
       " '[IT소식] 카카오미니, 멜론에서 저렴하게 Get!',\n",
       " '카카오미니, AI스피커시장 진출',\n",
       " \"네이버가 만드는 스마트 스피커, '프렌즈' 스피커 가격 및 스펙 정보\",\n",
       " '네이버 인공지능 스피커 프렌즈 클로바 실사용 후기',\n",
       " '[인공지능스피커 스펙비교] 카카오미니 vs. 라인프렌즈스피커',\n",
       " '카카오미니 파는곳 / 카카오인공지능스피커 판매처',\n",
       " '카카오, 네이버보다 마케팅 채널을 잘 쓰는 것 같다',\n",
       " '카카오 미니 사용 후기(feat.맘 아픈점)',\n",
       " '카카오미니 사용기',\n",
       " \"'카카오미니' 11월7일 정식출시, 정식판매 한해도 혜택이 펑펑~\",\n",
       " '카카오미니 스마트스피커 신박한 카카오프렌즈',\n",
       " '카카오미니 프렌즈 스마트스피커 사용기',\n",
       " '스마트 스피커 카카오미니, 카카오프렌즈와 함께 하는 일상 ',\n",
       " '카카오미니 후기 스마트스피커 카카오프렌즈와 완벽한조합',\n",
       " '카카오프렌즈를 만난 똑똑한 스마트스피커 카카오미니!!',\n",
       " '카카오미니 카카오프렌즈 스마트스피커 똑똑해요 :)',\n",
       " '카카오미니, 생활밀착형 인공지능스피커 솔직후기',\n",
       " '스마트 스피커 카카오미니!! 헤이 카카오로 카카오프렌즈를 불러주세요!',\n",
       " '스마트스피커 카카오미니 똑똑하고 귀여워',\n",
       " '스마트스피커 카카오미니(kakaomini), 놀라운 기능!',\n",
       " '카카오미니 스마트스피커 이젠 말로 하세요',\n",
       " '스마트스피커 카카오미니 후기',\n",
       " '카카오미니 친구 같은 스마트스피커 일상을 함께 해요.',\n",
       " '카카오프렌즈 카카오미니 스마트스피커 아내에게 첫 인공지능 스피커를 선물하다',\n",
       " '반가워, 스마트스피커 카카오미니 \"Hey Kakao!\"',\n",
       " '충동구매한 네이버 클로바 스피커, 대만족 후기',\n",
       " '카카오프렌즈 라이언과 인공지능이 결합된 카카오미니 (kakaomini) 스마트 스피커 사용해보니',\n",
       " '스마트 스피커 카카오미니 : 카카오프렌즈',\n",
       " '카카오미니 카카오프렌즈 라이언 스마트스피커 후기',\n",
       " '인공지능 스피커 카카오미니 후기 저 친구가 생겼어요',\n",
       " '업무 시간에 인공지능 스피커 ‘누구’와 대화를 나누어보았다',\n",
       " '카카오미니 사전예약 ai 스피커 늦은 개봉 후기',\n",
       " '2017.11.07 카카오미니 구입 / Kakao mini / 인공지능 스피커 / 멜론 정기 구독자',\n",
       " '카카오미니 가격, 카카오미니 멜론 정기결제 & 스트리밍 스피커 가격',\n",
       " '인공지능스피커(feat.블루투스)',\n",
       " '스마트홈을 이루어 줄 AI 스피커',\n",
       " '[인공지능 스피커] 카카오미니 AI 스피커 feat.라이언-솔직 후기',\n",
       " '인공지능 스피커 카카오 미니 득템!',\n",
       " '카카오미니, \"카카오톡과 연동한 AI스피커\"',\n",
       " '네이버-카카오 AI스피커 대결…첫 흥행은 카카오 승?',\n",
       " '네이버 프렌즈 스피커, 말 시켜보니(동영상)',\n",
       " '[카카오미니 사용후기] 인공지능 스피커 카카오미니 라이언 샀어요♥',\n",
       " '인공지능 스피커 비교, 카카오 미니 vs 네이버 웨이브 프렌즈 성능  비교 추천',\n",
       " '카카오미니 스피커 구매, 매장 재고 많이',\n",
       " '카카오미니, AI 스피커 간단 후기',\n",
       " '스피커들과 놀아보기(번외 테스트)',\n",
       " '한정판매 카카오미니(블루투스 스피커) 구입후기',\n",
       " \"[꿀빵]똑똑한 귀요미들 '카카오 미니'·'프렌즈' AI스피커 개봉기.avi\",\n",
       " '<카카오미니> 강남 카카오프렌즈 스토어에서 간단히 체험해본 소감',\n",
       " '프렌즈 스피커(인공지능, 클로바) 사용 후기',\n",
       " '인공지능 스피커 누구미니(NUGU) 개봉후기',\n",
       " '카카오미니 스피커 구입 후기',\n",
       " '카카오미니 개봉기~카카오 인공지능 스피커',\n",
       " '인공지능 AI스피커시대 , 카카오미니',\n",
       " 'CLOVA AI Speaker 프랜즈 브라운 블루투스 스피커 feat.서프라이즈선물',\n",
       " \"만듦새는 네이버, 쓰임새는 카카오…AI스피커 대결 '장군멍군'\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c07aa5fba8411db902bd9dd9af95b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2992), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('kakaomini_speaker.txt', 'w', encoding = 'utf8') as f:\n",
    "    title_list = []\n",
    "    publish_datetime_list = []\n",
    "    text_list = []\n",
    "    url_list = []\n",
    "    \n",
    "    for full_url in tqdm_notebook(total_url):\n",
    "        \n",
    "        try:       \n",
    "            # 실제 데이터가 들어있는 url 추출\n",
    "            real_url = get_realdata_url_naverblog(full_url)\n",
    "            r = requests.get(real_url)\n",
    "            soup = BeautifulSoup(r.text.encode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "            # 블로그 포스트 제목 추출\n",
    "            title = get_title_naverblog(soup)\n",
    "            if title in title_list:\n",
    "                continue\n",
    "            title_list.append(title)\n",
    "\n",
    "            # 블로그 작성일 추출\n",
    "            publish_datetime = det_publish_datetime_naverblog(soup)\n",
    "            publish_datetime_list.append(publish_datetime)\n",
    "\n",
    "            # 블로그 텍스트 추출\n",
    "            texts = get_texts_naverblog(soup)\n",
    "            text_list.append(texts)\n",
    "\n",
    "            # 중복 제거된 url\n",
    "            url_list.append(real_url)\n",
    "\n",
    "            #content = ' '.join(texts)\n",
    "            f.write(texts+'\\n') #결과를 text 파일에 저장합니다.\n",
    "            \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(text_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataframe.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'title': title_list, \n",
    "                        'publish_datetime': publish_datetime_list,\n",
    "                        'text': text_list, \n",
    "                        'url': real_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = df[['title', 'publish_datetime', 'text', 'url']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./kakomini_speaker_blog.csv',sep=',',encoding='UTF-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./kakaomini_speaker_blog.csv', encoding='UTF-16',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
